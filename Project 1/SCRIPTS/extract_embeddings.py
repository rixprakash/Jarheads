# -*- coding: utf-8 -*-
"""extract_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UkMxe1e2hkk_ywIeKFubiDyrVjS7tEL9
"""

import os
import gc
import torch
import numpy as np
import pandas as pd
import joblib
from transformers import BertTokenizer, BertModel
from sklearn.preprocessing import LabelEncoder

# ðŸš€ Ensure PyTorch GPU memory management is optimized
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
torch.cuda.empty_cache()

##############################
# 1. Load Data
##############################
def load_data():
    """Loads the processed dataset from GitHub."""
    GITHUB_CSV_URL = "https://raw.githubusercontent.com/rixprakash/Jarheads/main/Project%201/DATA/train_data.csv"
    df = pd.read_csv(GITHUB_CSV_URL)
    return df

##############################
# 2. Load Pretrained BERT Model
##############################
print("Loading BERT model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased").to(device)
print("BERT model loaded successfully!")

##############################
# 3. Convert Lyrics to BERT Embeddings (Batch Processing)
##############################
def get_bert_embeddings_batch(texts, batch_size=4):
    """Converts a batch of lyrics into BERT embeddings with proper GPU handling."""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        tokens = tokenizer(batch, return_tensors="pt", truncation=True, padding=True, max_length=512)
        tokens = {key: val.to(device) for key, val in tokens.items()}

        with torch.no_grad():
            output = bert_model(**tokens)

        batch_embeddings = output.last_hidden_state[:, 0, :].cpu().numpy()
        embeddings.extend(batch_embeddings)

    return np.array(embeddings)

##############################
# 4. Preprocess Data and Save
##############################
def preprocess_and_save_data():
    """Processes data, extracts embeddings, and saves them for later use."""
    df = load_data()
    print("Converting lyrics to BERT embeddings... This may take a few minutes.")

    X = get_bert_embeddings_batch(df["cleaned_lyrics"].tolist())

    # Encode labels
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(df["genre"])

    print("BERT embeddings extracted successfully!")

    # Save to disk
    np.save("X_embeddings.npy", X)
    np.save("y.npy", y_encoded)
    joblib.dump(label_encoder, "label_encoder.pkl")

    print("Saved embeddings & labels to disk.")

    # âœ… Free up GPU memory
    global bert_model  # Explicitly reference the global variable
    global tokenizer
    del bert_model
    del tokenizer
    torch.cuda.empty_cache()
    gc.collect()
    print("GPU memory cleared!")

# Run the embedding extraction
if __name__ == "__main__":
    preprocess_and_save_data()
