# -*- coding: utf-8 -*-
"""The_working_processe_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KXjIgt0Bepp1RGuA5-fDMOr7_fUnzhBL
"""

import os

# Remove the default NLTK data directory to avoid conflicts
os.system("rm -rf /root/nltk_data")

# Force NLTK to use /usr/local/share/nltk_data
os.environ["NLTK_DATA"] = "/usr/local/share/nltk_data"

import nltk

# Set a consistent NLTK data path
nltk.data.path = ["/usr/local/share/nltk_data"]

print("NLTK data path:", nltk.data.path)

# Remove any corrupted Punkt files (optional)
os.system("rm -rf /usr/local/share/nltk_data/tokenizers")

# Reinstall the Punkt tokenizer and other resources into /usr/local/share/nltk_data
nltk.download("punkt", download_dir="/usr/local/share/nltk_data")
nltk.download("punkt_tab", download_dir="/usr/local/share/nltk_data")
nltk.download("wordnet", download_dir="/usr/local/share/nltk_data")
nltk.download("stopwords", download_dir="/usr/local/share/nltk_data")

print("✅ Successfully reinstalled Punkt, punkt_tab, wordnet, and stopwords!")

from nltk.tokenize import word_tokenize, sent_tokenize

# Test tokenization
sentence = "This is a test sentence."
tokens = word_tokenize(sentence)
sentences = sent_tokenize(sentence)

print("✅ Word tokenization successful!", tokens)
print("✅ Sentence tokenization successful!", sentences)

# -----------------------------
import pandas as pd
import numpy as np
import re
import string
# import swifter  # Uncomment if you decide to use it for faster .apply()
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

# (No need to append another path since we've set it above)

# Load dataset from GitHub
GITHUB_CSV_URL = "https://raw.githubusercontent.com/rixprakash/Jarheads/main/Project%201/DATA/tcc_ceds_music.csv"
df = pd.read_csv(GITHUB_CSV_URL)

# Drop missing values and ensure all lyrics are strings
df.dropna(subset=["lyrics", "genre"], inplace=True)
df["lyrics"] = df["lyrics"].astype(str)

# Use NLTK's stopwords corpus
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print("✅ Stopwords successfully loaded!")

lemmatizer = WordNetLemmatizer()

def clean_text(text):
    """Cleans song lyrics by removing special characters, stopwords, and applying lemmatization."""
    if not isinstance(text, str) or text.strip() == "":
        return ""

    text = text.lower()  # Convert to lowercase
    text = text.replace("\n", " ")  # Replace line breaks with space
    text = re.sub(r"\d+", "", text)  # Remove numbers
    text = text.translate(str.maketrans("", "", string.punctuation))  # Remove punctuation
    tokens = word_tokenize(text)  # Tokenize words

    # Remove stopwords & lemmatize each token
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

    return " ".join(tokens)

# Process the lyrics
df["cleaned_lyrics"] = df["lyrics"].apply(clean_text)

# Save cleaned dataset
df[["cleaned_lyrics", "genre"]].to_csv("processed_data.csv", index=False)
print("✅ Processed dataset saved as processed_data.csv")

# Feature extraction using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf.fit_transform(df["cleaned_lyrics"])

# Save TF-IDF model
joblib.dump(tfidf, "tfidf_vectorizer.pkl")
print("✅ TF-IDF vectorizer saved as tfidf_vectorizer.pkl")