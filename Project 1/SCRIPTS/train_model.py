# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FMGLlpk2-prv7qK2e8rofPy1kjLRqXXN
"""

import numpy as np
import joblib
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
import matplotlib.pyplot as plt
import seaborn as sns
import urllib.request

# Step 1: Load Data from GitHub
X_url = "https://raw.githubusercontent.com/rixprakash/Jarheads/main/Project%201/OUTPUT/X_embeddings.npy"
y_url = "https://raw.githubusercontent.com/rixprakash/Jarheads/main/Project%201/OUTPUT/y.npy"
label_url = "https://raw.githubusercontent.com/rixprakash/Jarheads/main/Project%201/OUTPUT/label_encoder.pkl"

urllib.request.urlretrieve(X_url, "X_embeddings.npy")
urllib.request.urlretrieve(y_url, "y.npy")
urllib.request.urlretrieve(label_url, "label_encoder.pkl")

X = np.load("X_embeddings.npy", allow_pickle=True)
y = np.load("y.npy", allow_pickle=True)
label_encoder = joblib.load("label_encoder.pkl")

print(f" Loaded embeddings! X shape: {X.shape}, y shape: {y.shape}")

# Step 2: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Train Logistic Regression Model
print("\nTraining Logistic Regression Model...")
logreg_model = LogisticRegression(max_iter=5000, solver="lbfgs")
logreg_model.fit(X_train, y_train)

y_pred_logreg = logreg_model.predict(X_test)
print("\nLogistic Regression Results:")
print(classification_report(y_test, y_pred_logreg))

# Save Logistic Regression Model
joblib.dump(logreg_model, "logistic_regression_bert.pkl")
print("Logistic Regression model saved!")

# Step 4: Train Neural Network Model
def build_neural_network(input_dim, num_classes):
    model = Sequential([
        Input(shape=(input_dim,)),
        Dense(512, activation="relu"),
        Dropout(0.3),
        Dense(256, activation="relu"),
        Dropout(0.3),
        Dense(128, activation="relu"),
        Dropout(0.3),
        Dense(num_classes, activation="softmax")
    ])
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    return model

num_classes = len(label_encoder.classes_)

print("\nTraining Neural Network...")
nn_model = build_neural_network(X_train.shape[1], num_classes)

history = nn_model.fit(X_train, y_train, epochs=15, batch_size=4, validation_data=(X_test, y_test), verbose=1)

y_pred_nn = np.argmax(nn_model.predict(X_test), axis=1)
print("\nNeural Network Results:")
print(classification_report(y_test, y_pred_nn))

# Save Neural Network Model
nn_model.save("neural_network_bert.h5")
print("Neural Network model saved!")

# Step 5: Plot Confusion Matrices
def plot_confusion_matrix(y_test, y_pred, model_name, label_encoder):
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8,6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=label_encoder.classes_,
                yticklabels=label_encoder.classes_)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(f"Confusion Matrix - {model_name}")
    plt.show()

plot_confusion_matrix(y_test, y_pred_logreg, "Logistic Regression (BERT)", label_encoder)
plot_confusion_matrix(y_test, y_pred_nn, "Neural Network (BERT)", label_encoder)

print("\n Training Complete! Models and confusion matrices are ready.")